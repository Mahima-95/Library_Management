*****************map reduce max value

10.5.3.143

cd mahima

*****************hbase
********************************************************
for hbase just login as xavient on 10.5.2.108
sudo -i
1@34567b
su hdfs
hbase shell
list \\ command used to show created tables
scan 'emp' \\ check its rows with values
is_disabled 'emp' \\ check table is enabled or not
disabe 'emp' \\ to disable table
drop 'emp' \\ to delete table

get 'emp', 'row1' \\ to read row1 
get 'emp', 'row1', 'personal data:name' // to get specific column

need to disable table first for delete
create 'emp' , 'personal data'   //to create table if want

put 'emp' , 'row1', 'personal data:id','1' //to create data
put 'emp' , 'row1', 'personal data:name','mahima'

put 'emp','row1','personal data:name','Delhi' \\ if want to update value of name column from mahima to delhi


 delete 'emp', 'row1', 'personal data:name', 1496731128260 \\delete specific column with timestamp
deleteall 'emp1', 'row1' \\ delete full row

exists 'emp' \\ to check table is exists or not

count 'emp' \\ count total rows in this table

truncate 'emp' \\ to disable table, drop table and recreate table without data.
********************************************************
****************pig

 pig -x mapreduce \\ to enter in pig with map reduce
 cd  usr/hdp/2.6.0.3-8/pig/bin in 143
 and csv file into tmp/Mahima location
 
 10.5.2.108
login as root
cd /
 cd usr/hdp/2.6.0.3-8/pig/bin
nano wc.pig

input1 = LOAD 'hdfs://xtlinno1vhdtsg1.xavient.com:8020/tmp/Mahima/pigfile.csv'
                        using PigStorage(',');
dump input1;

ctrl+o
enter
ctrl+x


10.5.3.143
login as root

cd /
 cd usr/hdp/2.6.0.3-8/pig/bin
 
  pig -x mapreduce
exec wc.pig \\ for output




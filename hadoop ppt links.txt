Hadoop makes it possible to run applications on systems with thousands of commodity hardware nodes, and to handle thousands of terabytes of data.
Hadoop was created by computer scientists Doug Cutting and Mike Cafarella in 2006 to support distribution for the Nutch search engine. It was inspired by Google's MapReduce, a software framework in which an application is broken down into numerous small parts. Any of these parts, which are also called fragments or blocks, can be run on any node in the cluster.

solr is like a search engine for fast searching.
hadoop has different directory for input , output and for  data...

Hadoop-cluster- http://searchbusinessanalytics.techtarget.com/definition/Hadoop-cluster
mapreduce- https://hortonworks.com/apache/mapreduce/
https://dzone.com/articles/how-hadoop-mapreduce-works
https://hortonworks.com/apache/mapreduce/#section_2
https://dzone.com/articles/hadoop-basics-creating
https://www.dezyre.com/hadoop-tutorial/hadoop-mapreduce-tutorial-
http://www.folkstalk.com/2013/09/big-data-hadoop-architecture-components-tutorial.html	
why hadoop important in bigdata- http://blog.bigdataweek.com/2016/08/01/hadoop-important-handling-big-data/
jsp & servlet- https://www.javacodegeeks.com/2013/08/file-upload-example-in-servlet-and-jsp.html
https://doc.lucidworks.com/lucidworks-hdpsearch/2.5/Guide-Banana.html#_banana-dashboards
Linux links:-
http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-6.html
http://www.thegeekstuff.com/2010/11/50-linux-commands/?utm_source=feedburner

sql & no sql :-http://www.thegeekstuff.com/2014/01/sql-vs-nosql-db/?utm_source=tuicool


1.	Why we need Hadoop
2.	What is Big Data
3.	Hadoop Cluster (HDP overview)
4.	How to Connect to Hadoop Cluster
5.	Hadoop Installation
6.	Hadoop Architecture
7.	Introduction to HDFS, Map-Reduce, YARN, Flume, Sqoop, Hive and Oozie
8.	NoSQL and Hbase
9.	HBase Architecture
10.	Interact HBase using command Line
